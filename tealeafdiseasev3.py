# -*- coding: utf-8 -*-
"""TeaLeafDiseaseV3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cOvlHsCf5dq1SFRnqTAL44m_cC6NW2Q5
"""

from google.colab import drive
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
drive.mount('/content/drive')
data_dir = '/content/drive/MyDrive/Tea_leaf_disease'

IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Create an ImageDataGenerator for data augmentation and preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2 # Use 20% of data for validation
)

# Assuming your `data_dir` variable (from the previous cell) points to your image dataset
train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# Get the number of classes from the generator
num_classes = train_generator.num_classes
print(f"Detected {num_classes} classes.")

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Load the pre-trained MobileNetV2 model without the top classification layer
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))

# Freeze the base model layers so they are not updated during training
for layer in base_model.layers:
    layer.trainable = False

# Add a custom classification head on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x) # Add a dense layer
predictions = Dense(num_classes, activation='softmax')(x) # Output layer for classification

model = Model(inputs=base_model.input, outputs=predictions)

model.summary()

from tensorflow.keras.optimizers import Adam

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
# It's recommended to train for more epochs, this is just an example
history = model.fit(
    train_generator,
    epochs=25,
    validation_data=validation_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_steps=validation_generator.samples // BATCH_SIZE
)

print("Model training complete!")

import numpy as np
from tensorflow.keras.preprocessing import image
import os
from google.colab import files

# Prompt user to upload an image file
uploaded = files.upload()

# Get the filename of the uploaded file
if uploaded:
    for fn in uploaded.keys():
        local_image_path = fn
        print(f"User uploaded file: {local_image_path}")

    # Load and preprocess the image
    img = image.load_img(local_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0) # Create a batch dimension
    img_array = img_array / 255.0 # Rescale the image like the training data

    # Make a prediction
    predictions = model.predict(img_array)

    print(f"Raw prediction probabilities: {predictions}")

    # Interpret the prediction
    class_names = list(train_generator.class_indices.keys())
    predicted_class_index = np.argmax(predictions)
    predicted_class_name = class_names[predicted_class_index]
    confidence = predictions[0][predicted_class_index] * 100

    print(f"Predicted class: {predicted_class_name} with {confidence:.2f}% confidence.")

    if num_classes == 1:
        print("\nReminder: The model was trained with data from a directory that contained only one effective class.")
        print("If you intended multiple classes, please ensure your dataset is organized into separate subdirectories for each class.")
else:
    print("No file was uploaded.")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# 1. Plotting Training and Validation Accuracy and Loss

# Get the accuracy and loss values from the history object
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc))

plt.figure(figsize=(12, 6))

# Plot Training and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

# Plot Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

print("----------------------------------------------------")

# 2. Generating Confusion Matrix

# Reset the validation generator to ensure correct order of samples
validation_generator.reset()

# Get true labels for the validation set
y_true = validation_generator.classes[validation_generator.index_array]

# Get predictions from the model on the validation data
# Note: validation_steps was used in model.fit, so ensure we predict on the same number of steps

# Calculate steps for prediction to cover all validation samples
num_val_samples = validation_generator.samples
val_steps = int(np.ceil(num_val_samples / BATCH_SIZE)) # Cast to int

y_pred_raw = model.predict(validation_generator, steps=val_steps, verbose=1)

# Get the predicted class indices
y_pred = np.argmax(y_pred_raw, axis=1)

# Get class names from the generator
class_labels = list(validation_generator.class_indices.keys())

# Ensure y_true and y_pred have the same length
# The generator might yield more batches than strictly needed, or fewer if not enough data.
# We should trim y_true and y_pred to match the actual number of samples processed.

# The number of samples successfully predicted will be len(y_pred)
# So, we should trim y_true to match this length.
if len(y_true) > len(y_pred):
    y_true = y_true[:len(y_pred)]
elif len(y_pred) > len(y_true):
    # This case should ideally not happen if val_steps is calculated correctly
    # but as a safeguard, we trim y_pred.
    y_pred = y_pred[:len(y_true)]

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plotting the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

print("Evaluation complete!")